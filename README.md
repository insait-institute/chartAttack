# ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation

> #### Jesus-German Ortiz-Barajas, Jonathan Tonglet, Vivek Gupta, and Iryna Gurevych
>  


This repository contains the code and dataset associated with the arXiv preprint: [ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation](#). The code is released under an **Apache 2.0** license. The Attackviz dataset is released under an **GPLv3** license.

<p align="center">
  <img width="100%" src="img/ChartAttack_v2.png" alt="ChartAttack framework"/>
</p>

## Abstract
Multimodal large language models (MLLMs) are increasingly used to automate chart generation from data tables, enabling efficient data analysis and reporting but also introducing new misuse risks. In this work, we introduce ChartAttack, a novel framework for evaluating how MLLMs can be misused to generate misleading charts at scale. ChartAttack injects misleaders into chart designs, aiming to induce incorrect interpretations of the underlying data. Furthermore, we create AttackViz, a chart question-answering (QA) dataset where each (chart specification, QA) pair is labeled with effective misleaders and their induced incorrect answers. Experiments in in-domain and cross-domain settings show that ChartAttack significantly degrades the QA performance of MLLM readers, reducing accuracy by an average of 19.6 points and 14.9 points, respectively. A human study further shows an average 20.2 point drop in accuracy for participants exposed to misleading charts generated by ChartAttack. Our findings highlight an urgent need for robustness and security considerations in the design, evaluation, and deployment of MLLM-based chart generation systems. We make our code and data publicly available.

## Setup

### Prerequisites

- Python 3.12.8
- Conda or similar environment manager
- CUDA 11.8

### Installation
1. **Clone the repository**:
   ```bash
   git clone https://github.com/insait-institute/chartAttack.git
   cd chartAttack
   ```
2. **Set up the environment**:
   ```bash
   # Create and activate conda environment
   conda create -n chartAttack python=3.12.8
   conda activate chartAttack
   
   # Install dependencies
   pip install -r requirements.txt

### Fine-tune the Demonstration selection module
```bash
cd src/demonstration_selection_module/
```

```python
python finetuning_sentence_transformer_multilabel_MNR_IRE.py \
    --dataset PlotQA \
    --dataset_version ef-anchor_postive_median_balanced \
    --chart_type h_bar \
    --partition train \
    --val_partition val \
    --batch_size 64 \
    --epochs 5 \
    --learning_rate 3e-5
```
### Misleader-generator module inference
```bash
cd src/misleader_generator_module
```

```python
python few_shot_attacker.py \
  --model_checkpoint Qwen/Qwen2.5-Coder-7B-Instruct \
  --dataset PlotQA \
  --partition test \
  --chart_type v_bar \
  --shots 5 \
  --zero_shot False \
  --version complete \
  --data_dir datasets \
  --prompt_dir prompts \
  --response_dir responses
```

## Citation
```bibtex
@article{chartAttack2026,
  title={ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation},
  author={Ortiz-Barajas, Jesus-German and Tonglet, Jonathan and Gupta, Vivek and Gurevych, Iryna},
  journal={arXiv preprint},
  year={2026}
}
```

## Disclaimer

This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication.